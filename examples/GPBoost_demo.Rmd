---
title: "GPBoost Explained Using Examples"
author: "Fabio Sigrist"
output:
  html_document:
    # code_folding: hide
    css: buttonstyle.css
    number_sections: true
    toc: true
---

<script src="hideOutput.js"></script>
<!-- See https://stackoverflow.com/questions/37755037/how-to-add-code-folding-to-output-chunks-in-rmarkdown-html-documents -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=7, fig.height=5)
library(reticulate)
run_python = TRUE
run_cv = TRUE
run_group_re = TRUE
cache = TRUE
```

In this document, we illustrate how the GPBoost library can be used through the R and Python packages to learn models, make predictions, and choose tuning parameters. Further, we also compare the GPBoost algorithm to standard gradient boosting with a square loss and to standard Gaussian process regression.

# Modeling Background
Both tree-boosting and Gaussian processes are techniques that achieve **state-of-the-art predictive accuracy**. Besides this, **tree-boosting** has the following advantages: 

* Automatic modeling of non-linearities, discontinuities, and complex high-order interactions
* Robust to outliers in and multicollinearity among predictor variables
* Scale-invariant to monotone transformations of the predictor variables
* Automatic handling of missing values in predictor variables

**Gaussian process** models have the following advantage:

* Probabilistic predictions which allows for uncertainty quantification

For the GPBoost algorithm, it is assumed that the **response variable (label) $y$ is the sum of a non-linear mean function $F(X)$ and so-called random effects $Zb$**:
 $$ y = F(X) + Zb + \xi,$$
where $\xi$ is an independent error term and $X$ are covariates (=features).

The **random effects** can consists of

- Gaussian processes (including random coefficient processes)
- Grouped random effects (including nested, crossed, and random coefficient effects)
- A sum of the above

The model is trained using the **GPBoost algorithm, where trainings means learning the covariance parameters** of the random effects and the **mean function F(X) using a tree ensemble**. In brief, the GPBoost algorithm is a boosting algorithm that iteratively learns the covariance parameters and adds a tree to the ensemble of trees using a gradient and/or a Newton boosting step. In the GPBoost library, covariance parameters can be learned using (accelerated) gradient descent or Fisher scoring. Further, trees are learned using the [LightGBM](https://github.com/microsoft/LightGBM/) library. See [Sigrist (2020)](http://arxiv.org/abs/2004.02653) for more details.


# Combined Tree-Boosting and Gaussian Process Example 
In the following, we start with an example where the random part of the model consists of a Gaussian process. After this, we also give a brief example when using grouped random effects instead of a Gaussian process.

## Data
First, you need to load your data. For simplicity and illustrational purposes, we use simulated data here. In the following, we simulate data (in R) and illustrate the data in the figure below.
<div class="fold s">
```{r simulate_data, results='hide', message=F, cache=cache}
# Simulate data
n <- 200 # number of training samples
set.seed(5)
cov_function <- "exponential"
sigma2_1 <- 0.25^2 # marginal variance of GP
rho <- 0.2 # range parameter
sigma2 <- 0.1^2 # error variance
# Gaussian process: sample on a dense grid but observe only partially
nx <- 30 # test data: number of grid points on each axis
x2=x1=rep((1:nx)/nx,nx)
for(i in 1:nx) x2[((i-1)*nx+1):(i*nx)]=i/nx
coords_test <- cbind(x1,x2)
# training locations (exlcude upper right rectangle)
coords <- matrix(runif(2)/2,ncol=2)
while (dim(coords)[1]<n) {
  coord_i <- runif(2) 
  if (!(coord_i[1]>=0.7 & coord_i[2]>=0.7)) {
    coords <- rbind(coords,coord_i)
  }
}
n_all <- nx^2 + n # total number of data points 
D <- as.matrix(dist(rbind(coords_test,coords)))
if(cov_function=="exponential"){
  Sigma <- exp(-D/rho)+diag(1E-10,n_all)
}else if (cov_function=="gaussian"){
  Sigma <- exp(-(D/rho)^2)+diag(1E-10,n_all)
}
C <- t(chol(Sigma))
b_all <- sqrt(sigma2_1)*C%*%rnorm(n_all)
b <- b_all[(nx^2+1):n_all] # observed GP
# Function for non-linear mean. Two covariates of which only one has an effect
f1d <- function(x) 2*(1/(1+exp(-(x-0.5)*20)))
X <- matrix(runif(2*n),ncol=2)
F_X <- f1d(X[,1]) # mean
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- F_X + b + xi # observed data
# test data
x <- seq(from=0,to=1,length.out=nx^2)
X_test <- cbind(x,rep(0,nx^2))
y_test <- f1d(X_test[,1]) + b_all[1:nx^2] + sqrt(sigma2) * rnorm(nx^2)
```
</div>

```{python export_data, include=FALSE}
coords = r.coords
X = r.X
y = r.y
coords_test = r.coords_test
X_test = r.X_test
y_test = r.y_test
```

```{r plot_data, echo=F, results='hide', message=F, cache=cache, fig.align='center', fig.cap='Illustration of data: Mean function F(X) used for simulation, latent Gaussian process b, and observed data y (bottom plots)'}
library(ggplot2)
library(viridis)
library(gridExtra)
x <- seq(from=0,to=1,length.out=200)
plot1 <- ggplot(data=data.frame(x=x,f=f1d(x)), aes(x=x,y=f)) + geom_line(size=1.5, color="darkred") +
  ggtitle("Mean function") + xlab("X") + ylab("F(X)")
plot2 <- ggplot(data = data.frame(s_1=coords_test[,1],s_2=coords_test[,2],b=b_all[1:nx^2]),aes(x=s_1,y=s_2,color=b)) + 
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("(Latent) Gaussian process (GP)")
plot3 <- ggplot(data=data.frame(x=X[,1],y=y), aes(x=x,y=y)) + geom_point() + 
  geom_line(data=data.frame(x=x,f=f1d(x)), aes(x=x,y=f), size=1.5, color="darkred") +
  ggtitle("Observed data vs. covariate") + xlab("X") + ylab("y")
plot4 <- ggplot(data = data.frame(s_1=coords[,1],s_2=coords[,2],y=y),aes(x=s_1,y=s_2,color=y)) + 
  geom_point(size=3) + scale_color_viridis(option = "B") + ggtitle("Observed data vs. GP locations")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```


## Training
**Training a model** is done by 

1. **Specifying the GP / random effects model as a `GPModel`** and, optionally, setting parameters for the optimization of the covariance parameters
2. **Training the GPBoost model by calling `gpboost` or, equivalently, `gpb.train` and passing the `GPModel` as an argument**

***In R***
```{r training, results='hold', message=F, cache=cache, eval=T}
# 1. Create Gaussian process / random effects model
library(gpboost)
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
# The properties of the optimizer for the Gaussian process or 
# random effects model can be set as follows.
# Depending on the data, a different learning rate (lr_cov) is better.
# You can also try optimizer_cov = "fisher_scoring" which implements a
# form of (quasi-) Newton's method (also called natural gradient descent in machine learning)
# and does not require to specify a learning rate.
# In this example, we use gradient descent instead of Fisher scoring since the 
# negative likelihood of the GP model is very flat (or potentially non-convex) in some iterations
# of the boosting algorithm. This can cause problems for Fisher scoring.
gp_params <- list(optimizer_cov = "gradient_descent", lr_cov = 0.05, trace = FALSE,
                  use_nesterov_acc = TRUE, acc_rate_cov = 0.5)
gp_model$set_optim_params(params=gp_params)

# 2. Train model
bst <- gpboost(data = X, label = y,
               gp_model = gp_model,
               nrounds = 52, learning_rate = 0.01,
               max_depth = 5, min_data_in_leaf = 10,
               objective = "regression_l2", verbose = 0)
# show the estimated covariance parameters of the GPModel
print("Estimated covariance parameters:")
summary(gp_model)
print("True values:")
print(c(sigma2,sigma2_1,rho))
```

***In Python***
<div class="fold s o">
```{python training_python, collapse=T, message=F, cache=cache, eval=run_python}
# 1. Create Gaussian process / random effects model
import gpboost as gpb
import numpy as np
gp_model = gpb.GPModel(gp_coords=coords, cov_function="exponential")
gp_model.set_optim_params(params={"optimizer_cov": "gradient_descent", "lr_cov": 0.05,
                                  "use_nesterov_acc": True, "acc_rate_cov": 0.5, "trace": False})

# 2. Train model
data_train = gpb.Dataset(X, y)
params = { 'objective': 'regression_l2', 'learning_rate': 0.01,
            'max_depth': 5, 'min_data_in_leaf': 10, 'verbose': 0 }
bst = gpb.train(params=params, train_set=data_train,
                gp_model=gp_model, num_boost_round=52)
print("Estimated covariance parameters:")
gp_model.summary()
```
</div>


## Prediction
**Prediction is done by calling the `predict` function** and passing the covariates (=features) for the tree ensemble and the features that define the Gaussian process or random effect, i.e. the prediction locations in our case. Note that the predictions for the tree ensemble and the Gaussian process are returned separately. I.e., one needs to sum them to obtain a single point prediction.

***In R***
```{r prediction, results='hold', message=F, cache=cache, fig.align='center', fig.cap='Prediction: Predicted (posterior) mean and prediction uncertainty (=standard deviation) of GP as well as predicted mean function F(X).'}
# make predictions
pred <- predict(bst, data = X_test, gp_coords_pred = coords_test, predict_cov_mat = TRUE)
# plot predictions
plot5 <- ggplot(data = data.frame(s_1=coords_test[,1],s_2=coords_test[,2],b=pred$random_effect_mean),aes(x=s_1,y=s_2,color=b)) +
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("Predicted GP mean")
plot6 <- ggplot(data = data.frame(s_1=coords_test[,1],s_2=coords_test[,2],b=sqrt(diag(pred$random_effect_cov))),aes(x=s_1,y=s_2,color=b)) +
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + labs(title="Predicted GP standard deviation", subtitle=" = prediction uncertainty")
plot7 <- ggplot(data=data.frame(x=X_test[,1],f=pred$fixed_effect), aes(x=x,y=f)) + geom_line(size=1) +
  geom_line(data=data.frame(x=x,f=f1d(x)), aes(x=x,y=f), size=1.5, color="darkred") +
  ggtitle("Predicted and true F(X)") + xlab("X") + ylab("y")
plot8 <- ggplot(data = data.frame(s_1=coords_test[,1],s_2=coords_test[,2],b=b_all[1:nx^2]),aes(x=s_1,y=s_2,color=b)) + geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("'True' GP and obs. locations") + geom_point(data = data.frame(s_1=coords[,1],s_2=coords[,2],y=y),aes(x=s_1,y=s_2),size=3,col="white", alpha=1, shape=43)
grid.arrange(plot8, plot5, plot6, plot7, ncol=2)
# sum the predictions of the trees and the GP
y_pred <- pred$fixed_effect + pred$random_effect_mean
print("Mean square error:")
mean((y_pred-y_test)^2)
```

***In Python***
<div class="fold s o">
```{python prediction_python, collapse=T, message=F, cache=cache, eval=run_python}
pred = bst.predict(data=X_test, gp_coords_pred=coords_test, predict_cov_mat=True)
y_pred = pred['fixed_effect'] + pred['random_effect_mean']
print("Mean square error (MSE):")
np.mean((y_pred-y_test)**2)
```
</div>


## Parameter tuning
### Number of boosting iterations
Boosting with trees as base learners has several tuning parameters. Arguably the most important one is the **number of boosting iterations** (=number of trees). For choosing this, you can use the **`gpb.cv` function which performs k-fold cross validation**. This can be done as shown in the following. A computationally cheaper alternative to full k-fold cross-validation is to pass a validation data set to `gpboost` or `gpb.train`. See the [**Python examples**](https://github.com/fabsig/GPBoost/tree/master/examples/python-guide) and [**R examples**](https://github.com/fabsig/GPBoost/tree/master/R-package/demo) for more details.

***In R***
```{r cv, results='show', message=F, cache=cache, eval=run_cv}
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
gp_params <- list(optimizer_cov = "gradient_descent", lr_cov = 0.05,
                  use_nesterov_acc = TRUE, acc_rate_cov = 0.5)
gp_model$set_optim_params(params=gp_params)
params <- list(learning_rate = 0.01, max_depth = 5,
               min_data_in_leaf = 10, objective = "regression_l2")
set.seed(1)
# Ignore the warning messages on non-convergence. Convergence is difficult to achive in the over-fitting phase when there is little signal left for the GP in the data when subtracting the mean function F(X).
cvbst <- gpb.cv(params = params, data = X, label = y,
                gp_model = gp_model, use_gp_model_for_validation = FALSE,
                nrounds = 100, nfold = 4, verbose = 0,
                eval = "l2", early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", cvbst$best_iter))
```

***In Python***
<div class="fold s o">
```{python cv_python, collapse=T, message=F, cache=cache, eval=run_python}
gp_model = gpb.GPModel(gp_coords=coords, cov_function="exponential")
gp_model.set_optim_params(params={"optimizer_cov": "gradient_descent", "lr_cov": 0.05,
                                  "use_nesterov_acc": True, "acc_rate_cov": 0.5, "trace": False})
data_train = gpb.Dataset(X, y)
params = { 'objective': 'regression_l2', 'learning_rate': 0.01,
            'max_depth': 5, 'min_data_in_leaf': 10, 'verbose': 0}
cvbst = gpb.cv(params=params, train_set=data_train,
               gp_model=gp_model, use_gp_model_for_validation=False,
               num_boost_round=100, early_stopping_rounds=5,
               nfold=4, verbose_eval=False, show_stdv=False, seed=1)
print("Best number of iterations: " + str(np.argmin(cvbst['l2-mean'])))
```
</div>

### Other tuning paramters
**Other tuning parameters** include the learning rate, the maximal tree depth, the minimal number of samples per leaf, and the number of leaves. These can be chosen, e.g., using k-fold cross-validation as shown in the following.

***In R***
<div class="fold s o">
```{r cv_full, eval=F}
# GPModel
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
gp_params <- list(optimizer_cov = "gradient_descent", lr_cov = 0.05,
                  use_nesterov_acc = TRUE, acc_rate_cov = 0.5)
gp_model$set_optim_params(params=gp_params)
# Find best test score by using k-fold cross validation considering a grid of possible tuning parameters
best_score <- 1e99
for(lr in c(0.1,0.05,0.01)){
  for(min_data_in_leaf in c(1,10)){
    for(max_depth in c(1,3,5)){
      set.seed(1)
      cvbst <- gpb.cv(params = list(objective = "regression_l2"), data = X, label =y,
                      gp_model = gp_model, use_gp_model_for_validation = FALSE
                      nrounds = 100, nfold = 4, early_stopping_rounds = 5, verbose=0,
                      min_data_in_leaf = min_data_in_leaf,max_depth = max_depth, learning_rate = lr)
      if (cvbst$best_score < best_score) {
        best_score = cvbst$best_score
        best_ntree <- cvbst$best_iter
        best_lr <- lr
        best_min_data_in_leaf <- min_data_in_leaf
        best_max_depth <- max_depth
      }
    }
  }
}
print(paste0("best_ntree: ", best_ntree, ", best_lr: ",
             best_lr, ", best_max_depth: ", best_max_depth,
             ", best_min_data_in_leaf: ", best_min_data_in_leaf))
```
</div>

## Comparison to standard gradient boosting with a squared loss and a linear Gaussian process model
In the following, we compare the GPBoost algorithm to using standard gradient boosting with a squared loss and to a linear Gaussian process model. For gradient boosting, we add the coordinates to the covariates (features) for the trees. Note that we need to allow for a much larger number of boosting iterations as the continuous spatial process is difficult to learn with discontinuous trees. For the linear Gaussian process, we include the covariates through a linear regression term. As the results below show, **both standard gradient boosting with a squared loss and a linear Gaussian process model result in considerably larger mean square errors (MSE) compared to the GPBoost algorithm**.

***In R***
```{r comparison, results='hold', message=F, cache=cache, eval=T}
# 1. Linear Gaussian process model
X1 <- cbind(rep(1,n),X)# Need to add an intercept term to the model matrix
gp_model <- fitGPModel(gp_coords = coords, cov_function = "exponential",
                        y = y, X = X1,
                        params = list(optimizer_cov = "gradient_descent",
                                      lr_cov = 0.05, use_nesterov_acc = TRUE,
                                      acc_rate_cov = 0.5))
X_test1 <- cbind(rep(1,dim(X_test)[1]),X_test)
print("Fitted linear Gaussian process model:")
summary(gp_model)
y_pred_linGP <- predict(gp_model, gp_coords_pred = coords_test,X_pred = X_test1)
cat("\nMSE of linear Gaussian process model: \n")
mean((y_pred_linGP$mu-y_test)^2)

# 2. Standard gradient boosting with a squared loss
XC <- cbind(X,coords)
params <- list(learning_rate = 0.01, max_depth = 5,
               min_data_in_leaf = 10, objective = "regression_l2")
set.seed(1)
# We need to allow for a much larger number of boosting iterations as 
# the smooth spatial process is difficult to learn with discontinuous trees
cvbst <- gpb.cv(data = XC, label= y, params = params,
                nrounds = 500, nfold = 4, verbose = 0,
                eval = "l2", early_stopping_rounds = 5)
bst <- gpboost(data = XC, label = y,
               nrounds = cvbst$best_iter, learning_rate = 0.01,
               max_depth = 5, min_data_in_leaf = 10,
               objective = "regression_l2", verbose = 0)
X_testC <- cbind(X_test,coords_test)
pred_l2boost <- predict(bst, data = X_testC)
cat("\nMSE of standard gradient boosting with a squared loss: \n")
mean((pred_l2boost-y_test)^2)
cat("\nCompare to the MSE of GPBoost: \n")
mean((y_pred-y_test)^2)
```

# Combined Tree-Boosting and Grouped Random Effects Example
In the following, we show how a non-linear mixed effects model can be trained using `gpboost`. We use a random effects model that includes two crossed grouped random effects as well as a random slope. 

We first simulate data and then show how to train the GPBoost model and make predictions.
<div class="fold s">
```{r simulate_group_data, results='hide', message=F, cache=cache}
# Simulate data
set.seed(1)
n <- 1000 # number of samples
m <- 25 # number of categories / levels for grouping variable
n_obs_gr <- n/m # number of sampels per group
group <- rep(1,n) # grouping variable for first random effect
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
group2 <- rep(1,n) # grouping variable for second crossed random effect
for(i in 1:m) group2[(1:n_obs_gr)+n_obs_gr*(i-1)] <- 1:n_obs_gr
sigma2_1 <- 1^2 # variance of first random effect
sigma2_2 <- 0.5^2 # variance of second random effect
sigma2_3 <- 0.75^2 # variance of random slope for first random effect
sigma2 <- 0.5^2 # error variance
# incidence matrixces relating grouped random effects to samples
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
Z2 <- model.matrix(rep(1,n)~factor(group2)-1) # incidence matrix for second random effect
x_rand_slope <- runif(n) # covariate data for random slope
Z3 <- diag(x_rand_slope) %*% Z1 # incidence matrix for random slope for first random effect
b1 <- sqrt(sigma2_1) * rnorm(m) # simulate random effects
b2 <- sqrt(sigma2_2) * rnorm(n_obs_gr) # second random effect
b3 <- sqrt(sigma2_3) * rnorm(m) # random slope for first random effect
b <- Z1%*%b1 + Z2%*%b2 + Z3%*%b3 # sum of all random effects
# Function for non-linear mean. Two covariates of which only one has an effect
f1d <- function(x) 2*(1/(1+exp(-(x-0.5)*20)))
X <- matrix(runif(2*n),ncol=2)
F_X <- f1d(X[,1]) # mean
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- F_X + b + xi # observed data
# test data
X_test <- cbind(seq(from=0,to=1,length.out=n),rep(0,n))
group_test = rep(1,n)
group2_test = rep(1,n)
x_rand_slope_test = rep(0,n)
```
</div>

***In R***
```{r train_group_model, results='hold', message=F, cache=cache, eval=run_group_re}
# 1. Create random effects model
gp_model <- GPModel(group_data = cbind(group,group2),
                    group_rand_coef_data = x_rand_slope,
                    ind_effect_group_rand_coef = 1)# the random slope is for the first random effect

# # Use cross-validation to find the optimal number of iterations (approx. 15 here)
# params <- list(learning_rate = 0.1, max_depth = 1,
#                min_data_in_leaf = 1, objective = "regression_l2")
# cvbst <- gpb.cv(params = params, data = X, label = y,
#                 gp_model = gp_model, use_gp_model_for_validation = FALSE,
#                 nrounds = 100, nfold = 4, verbose = 1,
#                 eval = "l2", early_stopping_rounds = 5)
# cvbst$best_iter

# 2. Train model
bst <- gpboost(data = X, label = y,
               gp_model = gp_model,
               nrounds = 15, learning_rate = 0.1,
               max_depth = 1, min_data_in_leaf = 1,
               objective = "regression_l2", verbose = 0)
print("Estimated variance parameters:")
summary(gp_model)
print("True values:")
print(c(sigma2,sigma2_1,sigma2_2,sigma2_3))

# 3. Make predictions
pred <- predict(bst, data = X_test, group_data_pred = cbind(group_test,group2_test),
                group_rand_coef_data_pred = x_rand_slope_test)

```