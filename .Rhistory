setwd("C:/Users/whsigris/Dropbox/HSLU/Projects/MixedBoost/GPBoost/helpers")
setwd("C:/Users/whsigris/Dropbox/HSLU/Projects/MixedBoost/GPBoost")
path_tests = paste0(getwd(),.Platform$file.sep,file.path("R-package","tests","testthat"))
test_dir(path_tests, reporter = "summary")
# Run tests locally
library(testthat)
library(gpboost)
path_tests = paste0(getwd(),.Platform$file.sep,file.path("R-package","tests","testthat"))
test_dir(path_tests, reporter = "summary")
# Function that simulates uniform random variables
sim_rand_unif <- function(n, init_c=0.1){
mod_lcg <- 134456 # modulus for linear congruential generator (random0 used)
sim <- rep(NA, n)
sim[1] <- floor(init_c * mod_lcg)
for(i in 2:n) sim[i] <- (8121 * sim[i-1] + 28411) %% mod_lcg
return(sim / mod_lcg)
}
# Function for non-linear mean
sim_friedman3=function(n, n_irrelevant=5){
X <- matrix(sim_rand_unif(4*n,init_c=0.24234),ncol=4)
X[,1] <- 100*X[,1]
X[,2] <- X[,2]*pi*(560-40)+40*pi
X[,4] <- X[,4]*10+1
f <- sqrt(10)*atan((X[,2]*X[,3]-1/(X[,2]*X[,4]))/X[,1])
X <- cbind(rep(1,n),X)
if(n_irrelevant>0) X <- cbind(X,matrix(sim_rand_unif(n_irrelevant*n,init_c=0.6543),ncol=n_irrelevant))
return(list(X=X,f=f))
}
f1d <- function(x) 1.5*(1/(1+exp(-(x-0.5)*20))+0.75*x)
sim_non_lin_f=function(n){
X <- matrix(sim_rand_unif(2*n,init_c=0.96534),ncol=2)
f <- f1d(X[,1])
return(list(X=X,f=f))
}
# Make plot ("manual test")
n <- 1000
m <- 100
sim_data <- sim_non_lin_f(n=n)
group <- rep(1,n) # grouping variable
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
b1 <- qnorm(sim_rand_unif(n=m, init_c=0.943242))
eps <- b1[group]
eps <- eps - mean(eps)
y <- sim_data$f + eps + 0.1^2*sim_rand_unif(n=n, init_c=0.32543)
gp_model <- GPModel(group_data = group)
bst <- gpboost(data = sim_data$X, label = y, gp_model = gp_model,
nrounds = 100, learning_rate = 0.05, max_depth = 6,
min_data_in_leaf = 5, objective = "regression_l2", verbose = 0,
leaves_newton_update = TRUE)
nplot <- 200
X_test_plot <- cbind(seq(from=0,to=1,length.out=nplot),rep(0.5,nplot))
pred <- predict(bst, data = X_test_plot, group_data_pred = rep(-9999,nplot))
x <- seq(from=0,to=1,length.out=200)
plot(x,f1d(x),type="l",lwd=3,col=2,main="Mean function and fitted function")
lines(X_test_plot[,1],pred$fixed_effect,col=4,lwd=3)
ntrain <- ntest <- 1000
n <- ntrain + ntest
# Simulate fixed effects
sim_data <- sim_friedman3(n=n, n_irrelevant=5)
f <- sim_data$f
X <- sim_data$X
# Simulate grouped random effects
sigma2_1 <- 0.6 # variance of first random effect
sigma2_2 <- 0.4 # variance of second random effect
sigma2 <- 0.1^2 # error variance
m <- 40 # number of categories / levels for grouping variable
# first random effect
group <- rep(1,ntrain) # grouping variable
for(i in 1:m) group[((i-1)*ntrain/m+1):(i*ntrain/m)] <- i
group <- c(group, group)
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
b1 <- sqrt(sigma2_1) * qnorm(sim_rand_unif(n=m, init_c=0.542))
# Second random effect
n_obs_gr <- ntrain/m# number of sampels per group
group2 <- rep(1,ntrain) # grouping variable
for(i in 1:m) group2[(1:n_obs_gr)+n_obs_gr*(i-1)] <- 1:n_obs_gr
group2 <- c(group2,group2)
Z2 <- model.matrix(rep(1,n)~factor(group2)-1)
b2 <- sqrt(sigma2_2) * qnorm(sim_rand_unif(n=n_obs_gr, init_c=0.2354))
eps <- Z1 %*% b1 + Z2 %*% b2
group_data <- cbind(group,group2)
# Error term
xi <- sqrt(sigma2) * qnorm(sim_rand_unif(n=n, init_c=0.756))
# Observed data
y <- f + eps + xi
# Signal-to-noise ratio of approx. 1
# var(f) / var(eps)
# Split in training and test data
y_train <- y[1:ntrain]
X_train <- X[1:ntrain,]
group_data_train <- group_data[1:ntrain,]
y_test <- y[1:ntest+ntrain]
X_test <- X[1:ntest+ntrain,]
f_test <- f[1:ntest+ntrain]
group_data_test <- group_data[1:ntest+ntrain,]
# Define random effects model
gp_model <- GPModel(group_data = group_data_train)
# CV for finding number of boosting iterations with use_gp_model_for_validation = FALSE
dtrain <- gpb.Dataset(X_train, label = y_train)
params <- list(learning_rate = 0.01,
max_depth = 6,
min_data_in_leaf = 5,
objective = "regression_l2",
has_gp_model = TRUE)
folds <- list()
for(i in 1:4) folds[[i]] <- as.integer(1:(ntrain/4) + (ntrain/4) * (i-1))
cvbst <- gpb.cv(params = params,
data = dtrain,
gp_model = gp_model,
nrounds = 100,
nfold = 4,
eval = "l2",
early_stopping_rounds = 5,
use_gp_model_for_validation = FALSE,
fit_GP_cov_pars_OOS = FALSE,
folds = folds,
verbose = 0)
expect_equal(cvbst$best_iter, 62)
expect_lt(abs(cvbst$best_score-1.020787), 1E-4)
# CV for finding number of boosting iterations with use_gp_model_for_validation = TRUE
cvbst <- gpb.cv(params = params,
data = dtrain,
gp_model = gp_model,
nrounds = 100,
nfold = 4,
eval = "l2",
early_stopping_rounds = 5,
use_gp_model_for_validation = TRUE,
fit_GP_cov_pars_OOS = FALSE,
folds = folds,
verbose = 0)
expect_equal(cvbst$best_iter, 62)
expect_lt(abs(cvbst$best_score-0.6402082), 1E-4)
ntrain <- ntest <- 1000
n <- ntrain + ntest
# Simulate fixed effects
sim_data <- sim_friedman3(n=n, n_irrelevant=5)
f <- sim_data$f
X <- sim_data$X
# Simulate grouped random effects
sigma2_1 <- 0.6 # variance of first random effect
sigma2_2 <- 0.4 # variance of second random effect
sigma2 <- 0.1^2 # error variance
m <- 40 # number of categories / levels for grouping variable
# first random effect
group <- rep(1,ntrain) # grouping variable
for(i in 1:m) group[((i-1)*ntrain/m+1):(i*ntrain/m)] <- i
group <- c(group, group)
n_new <- 3# number of new random effects in test data
group[(length(group)-n_new+1):length(group)] <- rep(99999,n_new)
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
b1 <- sqrt(sigma2_1) * qnorm(sim_rand_unif(n=length(unique(group)), init_c=0.542))
# Second random effect
n_obs_gr <- ntrain/m# number of sampels per group
group2 <- rep(1,ntrain) # grouping variable
for(i in 1:m) group2[(1:n_obs_gr)+n_obs_gr*(i-1)] <- 1:n_obs_gr
group2 <- c(group2,group2)
group2[(length(group2)-n_new+1):length(group2)] <- rep(99999,n_new)
Z2 <- model.matrix(rep(1,n)~factor(group2)-1)
b2 <- sqrt(sigma2_2) * qnorm(sim_rand_unif(n=length(unique(group2)), init_c=0.2354))
eps <- Z1 %*% b1 + Z2 %*% b2
group_data <- cbind(group,group2)
# Error term
xi <- sqrt(sigma2) * qnorm(sim_rand_unif(n=n, init_c=0.756))
# Observed data
y <- f + eps + xi
# Signal-to-noise ratio of approx. 1
# var(f) / var(eps)
# Split in training and test data
y_train <- y[1:ntrain]
X_train <- X[1:ntrain,]
group_data_train <- group_data[1:ntrain,]
y_test <- y[1:ntest+ntrain]
X_test <- X[1:ntest+ntrain,]
f_test <- f[1:ntest+ntrain]
group_data_test <- group_data[1:ntest+ntrain,]
dtrain <- gpb.Dataset(data = X_train, label = y_train)
dtest <- gpb.Dataset.create.valid(dtrain, data = X_test, label = y_test)
valids <- list(test = dtest)
params <- list(learning_rate = 0.01,
max_depth = 6,
min_data_in_leaf = 5,
objective = "regression_l2")
folds <- list()
for(i in 1:4) folds[[i]] <- as.integer(1:(ntrain/4) + (ntrain/4) * (i-1))
# Validation metrics for training data
# Default metric is "Negative log-likelihood" if there is only one training set
gp_model <- GPModel(group_data = group_data_train)
bst <- gpboost(data = X_train, label = y_train, gp_model = gp_model, verbose = 1,
objective = "regression_l2", train_gp_model_cov_pars=FALSE, nrounds=1)
record_results <- gpb.get.eval.result(bst, "train", "Negative log-likelihood")
# CV for finding number of boosting iterations with use_gp_model_for_validation = FALSE
gp_model <- GPModel(group_data = group_data_train)
cvbst <- gpb.cv(params = params,
data = dtrain,
gp_model = gp_model,
nrounds = 100,
nfold = 4,
eval = "l2",
early_stopping_rounds = 5,
use_gp_model_for_validation = FALSE,
fit_GP_cov_pars_OOS = FALSE,
folds = folds,
verbose = 0)
expect_equal(cvbst$best_iter, 58)
expect_lt(abs(cvbst$best_score-1.021776), 1E-4)
